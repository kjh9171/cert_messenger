import requests
from bs4 import BeautifulSoup
import telegram
import asyncio
import schedule
import time
from datetime import datetime, timedelta, timezone
import sys
import os
import urllib3
import html
import json
import logging
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# SSL ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ ì„¤ì •
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- ì„¤ì • ì •ë³´ ---
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN', '8458654696:AAFbyTsyeGw2f7OO9sYm3wlQiS5NY72F3J0')
CHAT_ID = os.getenv('CHAT_ID', '7220628007')

# ìˆ˜ì§‘ ëŒ€ìƒ URL (ì›ë³¸ ëª©ë¡ ìœ ì§€)
URLS = {
    "clien_news": "https://www.clien.net/service/board/park",
    "naver_yonhap": "https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&sid1=001&sid2=140&oid=001&isYeonhapFlash=Y",
    "boannews": "https://www.boannews.com/media/t_list.asp",
    "krcert": "https://krcert.or.kr/kr/bbs/list.do?menuNo=205020&bbsId=B0000133"
}

SENT_TITLES_FILE = 'sent_titles.json'

def load_sent_titles():
    if os.path.exists(SENT_TITLES_FILE) and os.path.isfile(SENT_TITLES_FILE):
        try:
            with open(SENT_TITLES_FILE, 'r') as f:
                return set(json.load(f))
        except Exception as e:
            logger.error(f"Failed to load sent titles: {e}")
    return set()

def save_sent_titles(titles):
    try:
        with open(SENT_TITLES_FILE, 'w') as f:
            json.dump(list(titles), f)
    except Exception as e:
        logger.error(f"Failed to save sent titles: {e}")

last_sent_titles = load_sent_titles()

def get_kst_now():
    """í•œêµ­ í‘œì¤€ì‹œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(timezone(timedelta(hours=9)))

def capture_article_image(url, filename):
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ì „ì²´ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤."""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1280,1024")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(30)
        driver.get(url)
        time.sleep(5) 
        driver.save_screenshot(filename)
        logger.info(f"Screenshot captured: {filename}")
        return filename
    except Exception as e:
        logger.error(f"ìº¡ì²˜ ì‹¤íŒ¨: {e}")
        return None
    finally:
        if driver: 
            driver.quit()

def fetch_data():
    """í´ë¦¬ì•™ì˜ ìœ ìš©í•œ ì‚¬ì´íŠ¸ì™€ ìƒˆë¡œìš´ ì†Œì‹ ê²Œì‹œíŒì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    all_content = []

    # 1. í´ë¦¬ì•™ ìœ ìš©í•œ ì‚¬ì´íŠ¸
    try:
        res = requests.get(URLS["clien_useful"], headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        # ê³µì§€ì‚¬í•­ ì œì™¸í•˜ê³  ì¼ë°˜ ê²Œì‹œê¸€ë§Œ ìˆ˜ì§‘ (ë³´í†µ ê³µì§€ì‚¬í•­ì€ .list_notice í´ë˜ìŠ¤ê°€ ìˆê±°ë‚˜ ìˆœì„œê°€ ì•ì„)
        # í•˜ì§€ë§Œ ê¸°ì¡´ ë¡œì§ì„ ë”°ë¼ ìƒìœ„ 5ê°œë¥¼ ê°€ì ¸ì˜´
        for item in soup.select('.list_content .list_item')[:5]:
            title_tag = item.select_one('.list_title .list_subject')
            if title_tag:
                all_content.append({
                    "source": "í´ë¦¬ì•™ ìœ ìš©í•œ ì‚¬ì´íŠ¸",
                    "title": title_tag.get_text().strip(),
                    "link": "https://www.clien.net" + title_tag['href']
                })
        logger.info(f"í´ë¦¬ì•™ ìœ ìš©í•œ ì‚¬ì´íŠ¸: {len([x for x in all_content if x['source'] == 'í´ë¦¬ì•™ ìœ ìš©í•œ ì‚¬ì´íŠ¸'])} items")
    except Exception as e:
        logger.error(f"í´ë¦¬ì•™ ìœ ìš©í•œ ì‚¬ì´íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # 2. í´ë¦¬ì•™ ìƒˆë¡œìš´ ì†Œì‹
    try:
        res = requests.get(URLS["clien_news"], headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        for item in soup.select('.list_content .list_item')[:5]:
            title_tag = item.select_one('.list_title .list_subject')
            if title_tag:
                all_content.append({
                    "source": "í´ë¦¬ì•™ ìƒˆë¡œìš´ ì†Œì‹",
                    "title": title_tag.get_text().strip(),
                    "link": "https://www.clien.net" + title_tag['href']
                })
        logger.info(f"í´ë¦¬ì•™ ìƒˆë¡œìš´ ì†Œì‹: {len([x for x in all_content if x['source'] == 'í´ë¦¬ì•™ ìƒˆë¡œìš´ ì†Œì‹'])} items")
    except Exception as e:
        logger.error(f"í´ë¦¬ì•™ ìƒˆë¡œìš´ ì†Œì‹ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # 3. ë„¤ì´ë²„ ì—°í•©ë‰´ìŠ¤ ì†ë³´
    try:
        res = requests.get(URLS["naver_yonhap"], headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        for item in soup.select('.list_body li')[:5]:
            title_tag = item.select_one('a')
            if title_tag:
                link = title_tag['href']
                if not link.startswith('http'): link = "https://news.naver.com" + link
                all_content.append({
                    "source": "ë„¤ì´ë²„ ì—°í•©ë‰´ìŠ¤ ì†ë³´",
                    "title": title_tag.get_text().strip(),
                    "link": link
                })
        logger.info(f"ë„¤ì´ë²„ ì—°í•©ë‰´ìŠ¤ ì†ë³´: {len([x for x in all_content if x['source'] == 'ë„¤ì´ë²„ ì—°í•©ë‰´ìŠ¤ ì†ë³´'])} items")
    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ ì—°í•©ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # 4. ë³´ì•ˆë‰´ìŠ¤
    try:
        res = requests.get(URLS["boannews"], headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        # ë³´ì•ˆë‰´ìŠ¤ëŠ” .news_list a.news_content ë˜ëŠ” a ë‚´ë¶€ì˜ spanìœ¼ë¡œ ì‹ë³„ ê°€ëŠ¥
        for item in soup.select('.news_list a')[:10]: # ë§í¬ê°€ ì—¬ëŸ¬ê°œì¼ ìˆ˜ ìˆì–´ ë„‰ë„‰íˆ íƒìƒ‰
            title_tag = item.select_one('span')
            if title_tag and item.get('href'):
                link = item['href']
                if not link.startswith('http'): link = "https://www.boannews.com" + link
                all_content.append({
                    "source": "ë³´ì•ˆë‰´ìŠ¤",
                    "title": title_tag.get_text().strip(),
                    "link": link
                })
        # ì¤‘ë³µ ì œê±° (a íƒœê·¸ê°€ ì—¬ëŸ¬ê°œ ì¡í ìˆ˜ ìˆìŒ)
        seen_links = set()
        temp_content = []
        for c in all_content:
            if c['source'] == "ë³´ì•ˆë‰´ìŠ¤":
                if c['link'] not in seen_links:
                    seen_links.add(c['link'])
                    temp_content.append(c)
            else:
                temp_content.append(c)
        all_content = temp_content
        logger.info(f"ë³´ì•ˆë‰´ìŠ¤: {len([x for x in all_content if x['source'] == 'ë³´ì•ˆë‰´ìŠ¤'])} items")
    except Exception as e:
        logger.error(f"ë³´ì•ˆë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # 5. KISA ë³´ì•ˆê³µì§€ (KRCERT)
    try:
        res = requests.get(URLS["krcert"], headers=headers, timeout=10, verify=False)
        soup = BeautifulSoup(res.text, 'html.parser')
        # .bbsList ëŒ€ì‹  .tbl table ë˜ëŠ” .board .tbl table ì‚¬ìš©
        for item in soup.select('.tbl table tbody tr')[:5]:
            title_tag = item.select_one('td.sbj a')
            if title_tag:
                link = title_tag['href']
                if not link.startswith('http'): link = "https://krcert.or.kr" + link
                all_content.append({
                    "source": "KISA ë³´ì•ˆê³µì§€",
                    "title": title_tag.get_text().strip(),
                    "link": link
                })
        logger.info(f"KISA ë³´ì•ˆê³µì§€: {len([x for x in all_content if x['source'] == 'KISA ë³´ì•ˆê³µì§€'])} items")
    except Exception as e:
        logger.error(f"KISA ë³´ì•ˆê³µì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return all_content

async def send_briefing(is_test=False):
    global last_sent_titles
    bot = telegram.Bot(token=TELEGRAM_TOKEN)
    data = fetch_data()
    now_str = get_kst_now().strftime('%Y-%m-%d %H:%M')

    # í…ŒìŠ¤íŠ¸ ì‹œ ìƒìœ„ 5ê°œ, ì¼ë°˜ ì‹œ ì‹ ê·œ ê¸°ì‚¬ë§Œ ì „ì†¡
    new_items = data[:5] if is_test else [d for d in data if d['link'] not in last_sent_titles]

    if not new_items:
        logger.info(f"[{now_str}] ìƒˆë¡œìš´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"[{now_str}] {len(new_items)} ê°œì˜ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.")

    for item in new_items:
        safe_title = html.escape(item['title'])
        report = f"<b>ğŸ“¢ {item['source']}</b>\n"
        report += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        report += f"ğŸ“Œ <b>{safe_title}</b>\n\n"
        report += f"ğŸ”— <a href='{item['link']}'>ì›ë¬¸ ë§í¬ ë³´ê¸°</a>\n"
        report += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        report += f"â° <i>ìˆ˜ì§‘ì¼ì‹œ: {now_str}</i>"

        temp_img = f"shot_{int(time.time())}.png"
        try:
            img_path = capture_article_image(item['link'], temp_img)
            if img_path and os.path.exists(img_path):
                with open(img_path, 'rb') as photo:
                    await bot.send_photo(chat_id=CHAT_ID, photo=photo, caption=report, parse_mode='HTML')
                os.remove(img_path)
            else:
                await bot.send_message(chat_id=CHAT_ID, text=report, parse_mode='HTML')
            
            last_sent_titles.add(item['link'])
            save_sent_titles(last_sent_titles)
            await asyncio.sleep(1)
        except Exception as e:
            logger.error(f"ì „ì†¡ ì˜¤ë¥˜: {e}")

def job_wrapper(is_test=False):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(send_briefing(is_test=is_test))
    finally:
        loop.close()

if __name__ == "__main__":
    logger.info("í†µí•© ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì‹œìŠ¤í…œ ì›ë³µ ê°€ë™ ì‹œì‘...")
    # ì´ˆê¸° ì‹¤í–‰ ì‹œ ì†ŒìŠ¤ë³„ ê¸°ì‚¬ í…ŒìŠ¤íŠ¸ ë°œì†¡
    job_wrapper(is_test=True) 
    
    schedule.every(5).minutes.do(job_wrapper)
    while True:
        schedule.run_pending()
        time.sleep(1)