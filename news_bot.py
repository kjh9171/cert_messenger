import requests
from bs4 import BeautifulSoup
import telegram
import asyncio
import schedule
import time
from datetime import datetime, timedelta, timezone
import sys
import os
import urllib3
import html
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- ì„¤ì • ì •ë³´ ---
TELEGRAM_TOKEN = '8458654696:AAFbyTsyeGw2f7OO9sYm3wlQiS5NY72F3J0'
CHAT_ID = '7220628007'

URLS = {
    "yonhap": "https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&sid1=001&sid2=140&oid=001&isYeonhapFlash=Y",
    "cisa_kev": "https://www.cvedetails.com/cisa-known-exploited-vulnerabilities/kev-1.html",
    "boannews": "https://www.boannews.com/media/list.asp",
    "clien_park": "https://www.clien.net/service/group/community",
    "ddanzi": "https://www.ddanzi.com/free",
    "mbc": "https://imnews.imbc.com/replay/2026/nwdesk/",
    "naver_stock": "https://stock.naver.com/",
    "ddanzi_news": "https://www.ddanzi.com/ddanziNews"
}

last_sent_titles = set()

def get_kst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def get_article_summary(url):
    """ê¸°ì‚¬ ì›ë¬¸ì—ì„œ ì²« 3ë¬¸ì¥ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    try:
        # ì‘ë‹µ ì‹œê°„ì„ 5ì´ˆë¡œ ì œí•œí•˜ì—¬ ì‹œìŠ¤í…œ ì§€ì—° ë°©ì§€
        res = requests.get(url, headers=headers, timeout=5, verify=False)
        res.encoding = 'utf-8' # ì¸ì½”ë”© ëª…ì‹œ
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë”´ì§€ì¼ë³´ ë³¸ë¬¸ ì˜ì—­ì˜ ë‹¤ì–‘í•œ ì„ íƒì ëŒ€ì‘
        content_area = soup.select_one('#content_1') or soup.select_one('.read_body') or soup.select_one('.view_content')
        
        if content_area:
            # ë¶ˆí•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸ë‚˜ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for s in content_area(['script', 'style']):
                s.decompose()
            
            text = content_area.get_text(separator=' ', strip=True)
            # ë¬¸ì¥ ë¶€í˜¸ ë’¤ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
            sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if len(s.strip()) > 5]
            summary = " ".join(sentences[:3])
            return summary if summary else "ë³¸ë¬¸ ìš”ì•½ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        print(f"ìš”ì•½ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return "ë¯¸ë¦¬ë³´ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜ì´ì§€ì…ë‹ˆë‹¤."

def capture_article_image(url, filename):
    """ì•ˆì •ì ì¸ ìº¡ì²˜ë¥¼ ìœ„í•œ íƒ€ê²Ÿ ì˜ì—­ ì„¤ì •"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1080,1200")

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(20)
        driver.get(url)
        time.sleep(3) # ë™ì  ìš”ì†Œ ë¡œë”© ëŒ€ê¸°
        
        # ìº¡ì²˜ ì‹¤íŒ¨ ë°©ì§€ë¥¼ ìœ„í•´ ë°”ë”” ì „ì²´ ìº¡ì²˜ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        driver.save_screenshot(filename)
        return filename
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨: {e}")
        return None
    finally:
        if driver: driver.quit()

def fetch_data():
    """ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ ë³µêµ¬ ë° ì•ˆì •í™”"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    all_content = []

    # 5. ë”´ì§€ì¼ë³´ ììœ ê²Œì‹œíŒ ìˆ˜ì§‘ë¶€ (ë³µêµ¬)
    try:
        res = requests.get(URLS["ddanzi"], headers=headers, timeout=10, verify=False)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('table.fz_change tbody tr')
        count = 0
        for item in items:
            if count >= 3: break
            
            title_tag = item.select_one('.title a.link')
            if title_tag:
                link = title_tag['href']
                if not link.startswith('http'): link = "https://www.ddanzi.com" + link
                
                title = title_tag.get_text().strip()
                # ê´„í˜¸ ì•ˆì˜ ëŒ“ê¸€ ìˆ˜ ë“± ë¶ˆí•„ìš” í…ìŠ¤íŠ¸ ì •ì œ
                title = re.sub(r'\[\d+\]$', '', title).strip()
                
                all_content.append({
                    "source": "ë”´ì§€ê²Œì‹œíŒ", 
                    "title": title, 
                    "link": link,
                    "summary": get_article_summary(link),
                    "author": item.select_one('.author').get_text().strip() if item.select_one('.author') else "ìµëª…"
                })
                count += 1
    except Exception as e:
        print(f"ë”´ì§€ê²Œì‹œíŒ í¬ë¡¤ë§ ë³µêµ¬ ì‹¤íŒ¨: {e}")

    # ë‹¤ë¥¸ ë‰´ìŠ¤ ì†ŒìŠ¤ë“¤ë„ ìœ„ì™€ ìœ ì‚¬í•˜ê²Œ summary í•„ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ìœ ì§€ ê°€ëŠ¥
    return all_content

async def send_briefing(is_test=False):
    global last_sent_titles
    bot = telegram.Bot(token=TELEGRAM_TOKEN)
    now_str = get_kst_now().strftime('%Y-%m-%d %H:%M')
    
    data = fetch_data()
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œ ë¬´ì¡°ê±´ ì „ì†¡, ì¼ë°˜ ëª¨ë“œ ì‹œ ì¤‘ë³µ ì²´í¬
    new_items = data if is_test else [d for d in data if d['link'] not in last_sent_titles]

    if not new_items:
        print(f"[{now_str}] ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for item in new_items:
        # HTML íƒœê·¸ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ ì² ì €í•œ ì´ìŠ¤ì¼€ì´í”„
        safe_title = html.escape(item['title'])
        safe_summary = html.escape(item.get('summary', 'ë‚´ìš© ìš”ì•½ ì—†ìŒ'))
        
        report = f"<b>ğŸ”¥ {item['source']}</b>\n"
        report += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        report += f"ğŸ“Œ <b>{safe_title}</b>\n\n"
        report += f"ğŸ“ <b>3ë¬¸ì¥ ìš”ì•½:</b>\n"
        report += f"<blockquote>{safe_summary}</blockquote>\n\n"
        report += f"ğŸ”— <a href='{item['link']}'>ì›ë¬¸ ì½ê¸°</a>\n"
        report += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        report += f"â° <i>ìˆ˜ì§‘: {now_str}</i>"

        temp_img = f"shot_{int(time.time())}.png"
        try:
            img_path = capture_article_image(item['link'], temp_img)
            if img_path and os.path.exists(img_path):
                with open(img_path, 'rb') as photo:
                    await bot.send_photo(chat_id=CHAT_ID, photo=photo, caption=report, parse_mode='HTML')
                os.remove(img_path)
            else:
                await bot.send_message(chat_id=CHAT_ID, text=report, parse_mode='HTML')
            
            last_sent_titles.add(item['link'])
            await asyncio.sleep(2) # ì „ì†¡ ì•ˆì •ì„±ì„ ìœ„í•´ ëŒ€ê¸° ì‹œê°„ ìƒí–¥
        except Exception as e:
            print(f"ì „ì†¡ ë‹¨ê³„ ì˜¤ë¥˜: {e}")

def job_wrapper(is_test=False):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(send_briefing(is_test=is_test))
    finally:
        loop.close()

if __name__ == "__main__":
    print("ë‰´ìŠ¤ë´‡ ë³µêµ¬ ëª¨ë“œ ê°€ë™ ì¤‘...")
    # ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ë°œì†¡ì„ ìˆ˜í–‰í•˜ì—¬ ë³µêµ¬ í™•ì¸
    job_wrapper(is_test=True)
    
    schedule.every().hour.at(":00").do(job_wrapper)
    while True:
        schedule.run_pending()
        time.sleep(1)