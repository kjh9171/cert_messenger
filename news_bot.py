import requests
from bs4 import BeautifulSoup
import telegram
import asyncio
import schedule
import time
from datetime import datetime
import google.generativeai as genai

# --- ì„¤ì • ì •ë³´ ---
TELEGRAM_TOKEN = '8458654696:AAFbyTsyeGw2f7OO9sYm3wlQiS5NY72F3J0'
CHAT_ID = '7220628007'
# ë„¤ì´ë²„ ë‰´ìŠ¤ 'IT/ê³¼í•™' ë° 'ì‚¬íšŒ' ì„¹ì…˜ ìœ„ì£¼ë¡œ íƒìƒ‰í•˜ê¸° ìœ„í•´ URL ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
NEWS_URLS = [
    "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=105&sid2=732", # ë³´ì•ˆ/í•´í‚¹
    "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=105&sid2=283", # ì»´í“¨í„°/AI
    "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=102&sid2=249"  # ì‚¬ê±´ì‚¬ê³ 
]

GEMINI_API_KEY = 'AIzaSyA1kHWHYG8MUHXh2aUaDho6WBeeyMSuBpM'
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# í‚¤ì›Œë“œ í•„í„°ë§ (ì •ë³´ë³´í˜¸, AI, í•´í‚¹, ê°œì¸ì •ë³´, ì‚¬ê±´, ì‚¬ê³ )
TARGET_KEYWORDS = ['ì •ë³´ë³´í˜¸', 'AI', 'ì¸ê³µì§€ëŠ¥', 'í•´í‚¹', 'ê°œì¸ì •ë³´', 'ë³´ì•ˆ', 'ìœ ì¶œ', 'ì‚¬ê±´', 'ì‚¬ê³ ', 'í”¼ìŠµ', 'ê²½ì°°', 'ìˆ˜ì‚¬']
last_news_titles = set()

def fetch_filtered_news():
    """ì§€ì •ëœ ì„¹ì…˜ì—ì„œ í‚¤ì›Œë“œì— ë§ëŠ” ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/110.0.0.0"}
    filtered_news = []
    
    for url in NEWS_URLS:
        try:
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ëª©ë¡ êµ¬ì¡° (ì„¹ì…˜ë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ìœ ì—°í•˜ê²Œ ì„ íƒ)
            articles = soup.select('.list_body li') or soup.select('.newsct_list li')
            
            for article in articles:
                title_tag = article.select_one('a')
                if title_tag:
                    title = title_tag.get_text().strip()
                    link = title_tag['href']
                    if not title or len(title) < 5: continue
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ì‚¬
                    if any(keyword in title for keyword in TARGET_KEYWORDS):
                        if not link.startswith('http'):
                            link = "https://news.naver.com" + link
                        filtered_news.append({"title": title, "link": link})
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì˜¤ë¥˜({url}): {e}")
    
    # ì¤‘ë³µ ì œê±° (ì—¬ëŸ¬ ì„¹ì…˜ì— ê±¸ì¹œ ë‰´ìŠ¤ ë°©ì§€)
    unique_news = {n['title']: n for n in filtered_news}.values()
    return list(unique_news)

async def generate_comprehensive_summary(news_list):
    """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ë²ˆì— ì¢…í•© ìš”ì•½ ìƒì„±"""
    if not news_list:
        return ""
    
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        news_context = "\n".join([f"- {n['title']}" for n in news_list])
        
        prompt = f"""
        ë‹¹ì‹ ì€ ì •ë³´ë³´í˜¸ ë° ì‚¬íšŒ ì´ìŠˆ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
        ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ì½ê³ , í˜„ì¬ì˜ ì£¼ìš” íë¦„ì„ 3ë¬¸ì¥ ì´ë‚´ë¡œ ì¢…í•© ë¶„ì„í•˜ì„¸ìš”.
        - ê°œë³„ ê¸°ì‚¬ ìš”ì•½ì´ ì•„ë‹Œ ì „ì²´ì ì¸ 'íŠ¸ë Œë“œ'ì™€ 'ì£¼ì˜ì‚¬í•­' ìœ„ì£¼ë¡œ ì‘ì„±í•  ê²ƒ.
        - ë¬¸ì²´ëŠ” '~í•¨', '~ì„'ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ëë‚¼ ê²ƒ.
        - ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë‚˜ ë§ˆí¬ë‹¤ìš´(**)ì€ ì œì™¸í•  ê²ƒ.

        [ë‰´ìŠ¤ ëª©ë¡]
        {news_context}
        """
        
        response = await asyncio.to_thread(model.generate_content, prompt)
        return f"ğŸ’¡ <b>ì¢…í•© ë¶„ì„:</b> {response.text.strip()}\n"
    except Exception as e:
        print(f"ì¢…í•© ìš”ì•½ ì—ëŸ¬: {e}")
        return ""

async def analyze_and_report():
    """ë‰´ìŠ¤ í•„í„°ë§ ë° ì¢…í•© ë¦¬í¬íŠ¸ ì „ì†¡"""
    global last_news_titles
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    current_news = fetch_filtered_news()
    # ì‹ ê·œ ë‰´ìŠ¤ë§Œ ì¶”ì¶œ
    new_articles = [n for n in current_news if n['title'] not in last_news_titles]
    
    if not new_articles:
        print(f"[{now_str}] ê´€ë ¨ ì‹ ê·œ ì†ë³´ ì—†ìŒ.")
        return

    # 1. ì¢…í•© ìš”ì•½ ìƒì„± (ì¢…í•© ë¶„ì„ì€ í•œ ë²ˆë§Œ ìˆ˜í–‰)
    summary_text = await generate_comprehensive_summary(new_articles[:10])
    
    # 2. ë©”ì‹œì§€ êµ¬ì„±
    report = f"<b>ğŸ›¡ï¸ ë³´ì•ˆ/AI/ì‚¬ê±´ì‚¬ê³  ì£¼ìš” ì†Œì‹</b>\n"
    report += f"ğŸ“… {now_str} ê¸°ì¤€\n"
    report += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    
    if summary_text:
        report += f"{summary_text}\n"
        report += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    
    report += "<b>ğŸ“Œ ìµœì‹  ì†ë³´ ëª©ë¡</b>\n"
    for i, article in enumerate(new_articles[:8], 1): # ìµœëŒ€ 8ê°œ ë…¸ì¶œ
        report += f"{i}. <a href='{article['link']}'>{article['title']}</a>\n"
    
    # í…”ë ˆê·¸ë¨ ì „ì†¡
    bot = telegram.Bot(token=TELEGRAM_TOKEN)
    try:
        await bot.send_message(chat_id=CHAT_ID, text=report, parse_mode='HTML', disable_web_page_preview=True)
        # ì „ì†¡ ì„±ê³µ í›„ íƒ€ì´í‹€ ì—…ë°ì´íŠ¸
        last_news_titles.update([n['title'] for n in new_articles])
    except Exception as e:
        print(f"ì „ì†¡ ì˜¤ë¥˜: {e}")

def job_wrapper():
    asyncio.run(analyze_and_report())

# ë§¤ì‹œ 00ë¶„ ì‹¤í–‰
schedule.every().hour.at(":00").do(job_wrapper)

if __name__ == "__main__":
    print("íŠ¹í™” ë‰´ìŠ¤ ë¶„ì„ ë´‡ ê°€ë™ ì‹œì‘ (ë³´ì•ˆ/AI/ì‚¬ê±´ì‚¬ê³ )...")
    job_wrapper() 
    while True:
        schedule.run_pending()
        time.sleep(1)